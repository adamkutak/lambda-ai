COMPLETE:
Check for valid python code: use black formatting, seems to work perfectly (also added running the python code once over)
JSON returning: seems to work fine 99%? Very occasional we get weird errors with "incorrect delimiter" or similar.
- seems like the error handling can fix it though.
Create fastAPI function definition via templates. Don't generate it with AI.
- this half works. We generate the definition/decorator and give it to the AI.
turn some repetitive code into callable functions and add more class structure - (v1 DONE!)
test reliability, find more possible errors that can arise and patch them - (Done for now??)
JSON errors: implement 1-shot preprompting to improve performance - DONE, seems perfect.
If we fail to create the function, the code should try again starting from scratch (new message history) - DONE
automatically pip install the requirements. - DONE (builds requirements file for APIEnv)
one-shot preprompting WITH databases - DONE
Add support for making databases, to create stateful APIs (v1 done)
fix error: Internal Server Error. make it return the error to the AI. (v1 done)

TODO (generation improvement features):
Use Chain of thought reasoning for more complex functions. Break them down into smaller components
analyzing errors with a specialized AI instance? "explain what went wrong in this code with this error msg.
AI has trouble with somewhat simple database functions. Needs chain of thought and GPT-4
  - it tries to make highly compounded sql, which causes it to get things slightly wrong or miss a minor instruction
If the user is generating functions frequently with the same database, we could also add few-shot prompts of the past openai calls that worked correctly.
     - then, future output would be more likely to be correct. 

TODO (auto testing):
although we implemented a base version, it doesn't really work.
instead of generating inputs, we should only generate the sql and maybe outputs as well.

TODO (MAJOR STEPS):
create a virtual environment to containerize the application?
Build a tool that lets you upload external interface documentation with the necessary API keys.
- Generated functions can then incorporate these API's
use GPT-4 eventually for far better performance.
telemetry/logging of actions taken.

TODO (backend service):
Return schemas of the database instead of the database object,
    - Then, get rid of classes for the core tech. Convert the functions so that they 
      accept the schema of the database object instead. They don't need to be class functions anymore.

Generating a function might sometimes take a while (over 30 seconds):
  - user might get bored and change pages. 
  - we need to think of generating a function as a potential longrunning task
    that happens in the background. The new tool should popup once it is complete.
  - would be really nice to have a queue of functions being generated on the UI, 
    so the user knows where it is
  
Paths and such are a huge mess.

Bug: I need to track which ports are live, servers need some way of ensuring they are on an open port at all times.
 - for now, I'll have a special port for testing.

--------------------------------
 General roadmap of features to add
--------------------------------
First iteration MVP:
- Get Frontend calls to backend working (DONE)
- name slugging for user inputs (DONE)
- add user authentication/management
- add landing page with login screen
- figure out deployment (possible issues with core tech)
- user can't test SQL in UI
- ability to delete/undeploy things
- simple admin console to view database
--------------------------------
Second Iteration MVP:
- refactor backend classes
- improve generation features
- additional UI views
- ability to add external packages/APIs
- queue cards in UI to see the status of a generating API tool.
  - otherwise, if the user leaves the page they can't see if an error occured
--------------------------------
Other Features:
- On tool page: use tool in excel/python/others (copy link/code)
- create table from excel/csv
- download table as excel/csv
- modify table (add columns)
- perform operation on table without functions
- create new function by modifying existing function


SQL GEN BRANCH
--------------
goal: when a user creates a new tool they
      specify test cases to make sure that the tool works. 
      
      If that tool is stateful (has a db attached) then testing the output
      of the tool is not good enough. You need to also test if the database
      was updated correctly. 
      
      To do so, the user can specify a database test comprised of a setup
      statement and a post-run query and state.

      This can be a bit cumbersome, and our goal is to have an AI agent create
      the setup statement, the post-run query, and post-run state given a test
      case.

- Create new one shot prompt (lambdaai/prompts.py)
- Create new function call (lambdaai/gpt_function_calls.py)
- Create a /generate_test_sql route

Qs:
- where do we store this sql info?
  - right now we store the test-cases in the APIFunction Model
